[
    {
        "sample_id": 0,
        "problem_id": 1,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.01s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=relu_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17  -c /scratch/torch_ext/6/relu_cuda/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=relu_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/6/relu_cuda/cuda.cu -o cuda.cuda.o \n[3/3] c++ main.o cuda.cuda.o -shared  -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o relu_cuda.so\nLoad time for model llm: 135.88s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.08s\nName: llm, compile: False, runtime: 0.027 ms\nTime to eval model llm, compile=False: 1.52s\n",
            "stderr": "Using /scratch/torch_ext/6 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/6/relu_cuda...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/6/relu_cuda/build.ninja...\nBuilding extension module relu_cuda...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module relu_cuda...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.026545291766524315
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 1,
        "problem_id": 2,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=relu_native -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/torch_ext/1/relu_native/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=relu_native -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/1/relu_native/cuda.cu -o cuda.cuda.o \n[3/3] c++ main.o cuda.cuda.o -shared -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o relu_native.so\nLoad time for model llm: 136.17s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.27s\nName: llm, compile: False, runtime: 0.041 ms\nTime to eval model llm, compile=False: 1.93s\n",
            "stderr": "Using /scratch/torch_ext/1 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/1/relu_native...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/1/relu_native/build.ninja...\nBuilding extension module relu_native...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module relu_native...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.041254594922065735
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 2,
        "problem_id": 4,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.01s\nLoad time for model llm: 0.03s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.91s\nName: llm, compile: False, runtime: 0.069 ms\nTime to eval model llm, compile=False: 1.84s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.06858006864786148
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 3,
        "problem_id": 6,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.95s\nLoad time for model llm: 4.49s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 10.03s\nName: llm, compile: False, runtime: 7.895 ms\nTime to eval model llm, compile=False: 6.56s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 7.895161151885986
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 4,
        "problem_id": 8,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\nLoad time for model llm: 0.01s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0005971193313598633]\nTime to check correctness: 6.18s\nName: llm, compile: False, runtime: 0.969 ms\nTime to eval model llm, compile=False: 9.07s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0005971193313598633
                ],
                "runtime": 0.969424843788147
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 5,
        "problem_id": 9,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.07s\nLoad time for model llm: 228.46s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.79s\nName: llm, compile: False, runtime: 0.934 ms\nTime to eval model llm, compile=False: 3.53s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.9342394471168518
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 6,
        "problem_id": 10,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.24s\nLoad time for model llm: 12.01s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: False, Max Diff: [0.04216718301177025]\nTime to check correctness: 1.00s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": false,
                "max_diff": [
                    0.04216718301177025
                ]
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 7,
        "problem_id": 13,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.07s\nLoad time for model llm: 121.95s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.60s\nName: llm, compile: False, runtime: 0.304 ms\nTime to eval model llm, compile=False: 3.66s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.30362582206726074
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 8,
        "problem_id": 14,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.08s\nLoad time for model llm: 0.12s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 2.40s\nName: llm, compile: False, runtime: 8.666 ms\nTime to eval model llm, compile=False: 9.26s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 8.66572093963623
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 9,
        "problem_id": 15,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.09s\nLoad time for model llm: 136.15s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.27s\nName: llm, compile: False, runtime: 3.574 ms\nTime to eval model llm, compile=False: 5.60s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 3.574320077896118
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 10,
        "problem_id": 16,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.17s\nLoad time for model llm: 0.15s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [1.695007085800171e-07]\nTime to check correctness: 22.26s\nName: llm, compile: False, runtime: 12.789 ms\nTime to eval model llm, compile=False: 5.79s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    1.695007085800171e-07
                ],
                "runtime": 12.78923225402832
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 11,
        "problem_id": 17,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.01s\nLoad time for model llm: 0.01s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0011420845985412598]\nTime to check correctness: 0.89s\nName: llm, compile: False, runtime: 0.925 ms\nTime to eval model llm, compile=False: 4.95s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0011420845985412598
                ],
                "runtime": 0.924871563911438
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 12,
        "problem_id": 19,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\nLoad time for model llm: 0.87s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 7.19s\nName: llm, compile: False, runtime: 0.763 ms\nTime to eval model llm, compile=False: 5.49s\n",
            "stderr": "/opt/venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n  warnings.warn(\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.7634133100509644
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 13,
        "problem_id": 20,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.06s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=conv_bn_relu6_optimized -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17  -c /scratch/torch_ext/31/conv_bn_relu6_optimized/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=conv_bn_relu6_optimized -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/31/conv_bn_relu6_optimized/cuda.cu -o cuda.cuda.o \n[3/3] c++ main.o cuda.cuda.o -shared  -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o conv_bn_relu6_optimized.so\nLoad time for model llm: 108.21s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.35s\nName: llm, compile: False, runtime: 2.182 ms\nTime to eval model llm, compile=False: 5.87s\n",
            "stderr": "Using /scratch/torch_ext/31 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/31/conv_bn_relu6_optimized...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/31/conv_bn_relu6_optimized/build.ninja...\nBuilding extension module conv_bn_relu6_optimized...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module conv_bn_relu6_optimized...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 2.182462215423584
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 14,
        "problem_id": 21,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.23s\nLoad time for model llm: 4.42s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\n",
            "stderr": "/scratch/code/task_3-metr_21_5.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  @torch.cuda.amp.autocast()\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Traceback (most recent call last):\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 483, in identify_mutated_tensors\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(kernel, kwargs)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 187, in generate_ttir\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module = src.make_ir(options, codegen_fns, context)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 113, in make_ir\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1297, in ast_to_ttir\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     generator.visit(fn.parse())\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 359, in visit_Module\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ast.NodeVisitor.generic_visit(self, node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 415, in generic_visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(item)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 443, in visit_FunctionDef\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 687, in visit_If\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     cond = self.visit(node.test)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1137, in visit_BoolOp\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     lhs = self.visit(node.values[0])\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 771, in visit_Compare\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     raise self._unsupported(node, \"simultaneous multiple comparison is not supported\")\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] triton.compiler.errors.UnsupportedLanguageConstruct: at 26:19:\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Load bias\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     acc = tl.load(b_ptr + oc) if b_ptr is not None else 0.0\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Compute convolution\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     for ic in range(in_channels // groups):\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]         for kh in range(3):\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]             for kw in range(3):\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 ih = h * stride + kh - padding\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 iw = w * stride + kw - padding\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 if 0 <= ih < height and 0 <= iw < width:\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                    ^\nW0803 18:15:26.861000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] simultaneous multiple comparison is not supported\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Traceback (most recent call last):\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 483, in identify_mutated_tensors\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(kernel, kwargs)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 187, in generate_ttir\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module = src.make_ir(options, codegen_fns, context)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 113, in make_ir\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1297, in ast_to_ttir\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     generator.visit(fn.parse())\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 359, in visit_Module\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ast.NodeVisitor.generic_visit(self, node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 415, in generic_visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(item)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 443, in visit_FunctionDef\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 687, in visit_If\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     cond = self.visit(node.test)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1137, in visit_BoolOp\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     lhs = self.visit(node.values[0])\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 771, in visit_Compare\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     raise self._unsupported(node, \"simultaneous multiple comparison is not supported\")\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] triton.compiler.errors.UnsupportedLanguageConstruct: at 26:19:\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Load bias\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     acc = tl.load(b_ptr + oc) if b_ptr is not None else 0.0\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Compute convolution\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     for ic in range(in_channels // groups):\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]         for kh in range(3):\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]             for kw in range(3):\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 ih = h * stride + kh - padding\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 iw = w * stride + kw - padding\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 if 0 <= ih < height and 0 <= iw < width:\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                    ^\nW0803 18:15:26.873000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] simultaneous multiple comparison is not supported\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Traceback (most recent call last):\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 483, in identify_mutated_tensors\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(kernel, kwargs)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 187, in generate_ttir\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module = src.make_ir(options, codegen_fns, context)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 113, in make_ir\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1297, in ast_to_ttir\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     generator.visit(fn.parse())\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 359, in visit_Module\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ast.NodeVisitor.generic_visit(self, node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 415, in generic_visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(item)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 443, in visit_FunctionDef\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 687, in visit_If\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     cond = self.visit(node.test)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1137, in visit_BoolOp\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     lhs = self.visit(node.values[0])\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 771, in visit_Compare\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     raise self._unsupported(node, \"simultaneous multiple comparison is not supported\")\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] triton.compiler.errors.UnsupportedLanguageConstruct: at 26:19:\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Load bias\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     acc = tl.load(b_ptr + oc) if b_ptr is not None else 0.0\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Compute convolution\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     for ic in range(in_channels // groups):\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]         for kh in range(3):\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]             for kw in range(3):\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 ih = h * stride + kh - padding\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 iw = w * stride + kw - padding\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 if 0 <= ih < height and 0 <= iw < width:\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                    ^\nW0803 18:15:26.913000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] simultaneous multiple comparison is not supported\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Traceback (most recent call last):\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 483, in identify_mutated_tensors\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(kernel, kwargs)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 187, in generate_ttir\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module = src.make_ir(options, codegen_fns, context)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 113, in make_ir\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1297, in ast_to_ttir\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     generator.visit(fn.parse())\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 359, in visit_Module\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ast.NodeVisitor.generic_visit(self, node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 415, in generic_visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(item)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 443, in visit_FunctionDef\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 687, in visit_If\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     cond = self.visit(node.test)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1137, in visit_BoolOp\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     lhs = self.visit(node.values[0])\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 771, in visit_Compare\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     raise self._unsupported(node, \"simultaneous multiple comparison is not supported\")\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] triton.compiler.errors.UnsupportedLanguageConstruct: at 26:19:\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Load bias\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     acc = tl.load(b_ptr + oc) if b_ptr is not None else 0.0\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Compute convolution\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     for ic in range(in_channels // groups):\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]         for kh in range(3):\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]             for kw in range(3):\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 ih = h * stride + kh - padding\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 iw = w * stride + kw - padding\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 if 0 <= ih < height and 0 <= iw < width:\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                    ^\nW0803 18:15:26.922000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] simultaneous multiple comparison is not supported\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Traceback (most recent call last):\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 483, in identify_mutated_tensors\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(kernel, kwargs)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 187, in generate_ttir\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module = src.make_ir(options, codegen_fns, context)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 113, in make_ir\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1297, in ast_to_ttir\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     generator.visit(fn.parse())\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 359, in visit_Module\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ast.NodeVisitor.generic_visit(self, node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 415, in generic_visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(item)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 443, in visit_FunctionDef\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 687, in visit_If\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     cond = self.visit(node.test)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1137, in visit_BoolOp\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     lhs = self.visit(node.values[0])\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 771, in visit_Compare\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     raise self._unsupported(node, \"simultaneous multiple comparison is not supported\")\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] triton.compiler.errors.UnsupportedLanguageConstruct: at 26:19:\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Load bias\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     acc = tl.load(b_ptr + oc) if b_ptr is not None else 0.0\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Compute convolution\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     for ic in range(in_channels // groups):\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]         for kh in range(3):\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]             for kw in range(3):\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 ih = h * stride + kh - padding\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 iw = w * stride + kw - padding\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 if 0 <= ih < height and 0 <= iw < width:\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                    ^\nW0803 18:15:27.122000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] simultaneous multiple comparison is not supported\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Encountered an exception in identify_mutated_tensors, assuming every input is mutated\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] Traceback (most recent call last):\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 483, in identify_mutated_tensors\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module, ordered_tensor_names = generate_ttir(kernel, kwargs)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/torch/_higher_order_ops/triton_kernel_wrap.py\", line 187, in generate_ttir\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ttir_module = src.make_ir(options, codegen_fns, context)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/compiler.py\", line 113, in make_ir\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1297, in ast_to_ttir\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     generator.visit(fn.parse())\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 359, in visit_Module\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ast.NodeVisitor.generic_visit(self, node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 415, in generic_visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(item)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 443, in visit_FunctionDef\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 964, in visit_For\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit_compound_statement(node.body)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 351, in visit_compound_statement\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     self.visit(stmt)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 687, in visit_If\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     cond = self.visit(node.test)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1137, in visit_BoolOp\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     lhs = self.visit(node.values[0])\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 1204, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     ret = super().visit(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]           ^^^^^^^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/usr/lib/python3.12/ast.py\", line 407, in visit\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     return visitor(node)\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]            ^^^^^^^^^^^^^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]   File \"/opt/venv/lib/python3.12/site-packages/triton/compiler/code_generator.py\", line 771, in visit_Compare\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     raise self._unsupported(node, \"simultaneous multiple comparison is not supported\")\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] triton.compiler.errors.UnsupportedLanguageConstruct: at 26:19:\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Load bias\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     acc = tl.load(b_ptr + oc) if b_ptr is not None else 0.0\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     # Compute convolution\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]     for ic in range(in_channels // groups):\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]         for kh in range(3):\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]             for kw in range(3):\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 ih = h * stride + kh - padding\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 iw = w * stride + kw - padding\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] \nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                 if 0 <= ih < height and 0 <= iw < width:\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0]                    ^\nW0803 18:15:27.133000 30 torch/_higher_order_ops/triton_kernel_wrap.py:504] [0/0] simultaneous multiple comparison is not supported\nTraceback (most recent call last):\n  File \"/app/scripts/eval.py\", line 462, in <module>\n    main()\n  File \"/app/scripts/eval.py\", line 447, in main\n    ev.check_correctness()\n  File \"/app/scripts/eval.py\", line 219, in check_correctness\n    correct, max_diff = self.compare_output(baseline_output, comp_output)\n                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/scripts/eval.py\", line 189, in compare_output\n    correct = torch.allclose(\n              ^^^^^^^^^^^^^^^\nRuntimeError: The size of tensor a (112) must match the size of tensor b (110) at non-singleton dimension 3\n",
            "eval_results": {},
            "timed_out": false
        }
    },
    {
        "sample_id": 15,
        "problem_id": 22,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.04s\nLoad time for model llm: 30.74s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [1.862645149230957e-09]\nTime to check correctness: 11.10s\nName: llm, compile: False, runtime: 1.047 ms\nTime to eval model llm, compile=False: 5.22s\n",
            "stderr": "/opt/venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:167: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n  warnings.warn(\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    1.862645149230957e-09
                ],
                "runtime": 1.0473411083221436
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 16,
        "problem_id": 23,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.04s\nLoad time for model llm: 135.46s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [1.862645149230957e-09]\nTime to check correctness: 2.90s\nName: llm, compile: False, runtime: 0.929 ms\nTime to eval model llm, compile=False: 5.02s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    1.862645149230957e-09
                ],
                "runtime": 0.9292717576026917
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 17,
        "problem_id": 24,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.06s\nLoad time for model llm: 0.04s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: False, Max Diff: [1.576265811920166]\nTime to check correctness: 1.38s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": false,
                "max_diff": [
                    1.576265811920166
                ]
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 18,
        "problem_id": 25,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.49s\nLoad time for model llm: 0.01s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 2.97s\nName: llm, compile: False, runtime: 7.836 ms\nTime to eval model llm, compile=False: 6.19s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 7.836461067199707
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 19,
        "problem_id": 26,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\nLoad time for model llm: 0.10s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [8.445978164672852e-05]\nTime to check correctness: 0.99s\nName: llm, compile: False, runtime: 8.084 ms\nTime to eval model llm, compile=False: 9.07s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    8.445978164672852e-05
                ],
                "runtime": 8.084274291992188
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 20,
        "problem_id": 27,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\nLoad time for model llm: 0.06s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [1.991167664527893e-05]\nTime to check correctness: 1.86s\nName: llm, compile: False, runtime: 7.750 ms\nTime to eval model llm, compile=False: 6.22s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    1.991167664527893e-05
                ],
                "runtime": 7.750418186187744
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 21,
        "problem_id": 29,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.11s\nLoad time for model llm: 214.78s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [3.129243850708008e-07]\nTime to check correctness: 0.45s\nName: llm, compile: False, runtime: 5.806 ms\nTime to eval model llm, compile=False: 5.05s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    3.129243850708008e-07
                ],
                "runtime": 5.805877208709717
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 22,
        "problem_id": 30,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.44s\nLoad time for model llm: 136.48s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.63s\nName: llm, compile: False, runtime: 7.974 ms\nTime to eval model llm, compile=False: 7.95s\n",
            "stderr": "/opt/venv/lib/python3.12/site-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 7.974029541015625
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 23,
        "problem_id": 31,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=attention_ops -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/torch_ext/9/attention_ops/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=attention_ops -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/9/attention_ops/cuda.cu -o cuda.cuda.o \n/scratch/torch_ext/9/attention_ops/cuda.cu(35): warning #177-D: variable \"wid\" was declared but never referenced\n      const int wid = tid / 32;\n                ^\n\nRemark: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n\n/scratch/torch_ext/9/attention_ops/cuda.cu(36): warning #177-D: variable \"lane\" was declared but never referenced\n      const int lane = tid % 32;\n                ^\n\n[3/3] c++ main.o cuda.cuda.o -shared -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o attention_ops.so\nLoad time for model llm: 134.70s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [9.5367431640625e-07]\nTime to check correctness: 0.75s\nName: llm, compile: False, runtime: 36.562 ms\nTime to eval model llm, compile=False: 5.05s\n",
            "stderr": "Using /scratch/torch_ext/9 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/9/attention_ops...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/9/attention_ops/build.ninja...\nBuilding extension module attention_ops...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module attention_ops...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    9.5367431640625e-07
                ],
                "runtime": 36.562339782714844
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 24,
        "problem_id": 33,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.01s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=cuda_operations -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/torch_ext/23/cuda_operations/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=cuda_operations -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/23/cuda_operations/cuda.cu -o cuda.cuda.o \n[3/3] c++ main.o cuda.cuda.o -shared -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o cuda_operations.so\nLoad time for model llm: 121.86s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.06s\nName: llm, compile: False, runtime: 0.025 ms\nTime to eval model llm, compile=False: 1.38s\n",
            "stderr": "Using /scratch/torch_ext/23 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/23/cuda_operations...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/23/cuda_operations/build.ninja...\nBuilding extension module cuda_operations...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module cuda_operations...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.025096779689192772
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 25,
        "problem_id": 36,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.06s\nLoad time for model llm: 0.02s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 3.08s\nName: llm, compile: False, runtime: 23.112 ms\nTime to eval model llm, compile=False: 10.76s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 23.111867904663086
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 26,
        "problem_id": 37,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\nLoad time for model llm: 0.02s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 0.62s\nName: llm, compile: False, runtime: 23.864 ms\nTime to eval model llm, compile=False: 7.75s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 23.863750457763672
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 27,
        "problem_id": 38,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.06s\nLoad time for model llm: 116.23s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\n",
            "stderr": "Traceback (most recent call last):\n  File \"/app/scripts/eval.py\", line 462, in <module>\n    main()\n  File \"/app/scripts/eval.py\", line 447, in main\n    ev.check_correctness()\n  File \"/app/scripts/eval.py\", line 217, in check_correctness\n    comp_output = self.get_model_output(name)\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/app/scripts/eval.py\", line 175, in get_model_output\n    return model.to(self.locked_device)(*easy_to_device(self.inputs, self.locked_device))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/venv/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: Model.forward() takes 2 positional arguments but 4 were given\n",
            "eval_results": {},
            "timed_out": false
        }
    },
    {
        "sample_id": 28,
        "problem_id": 39,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.02s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=fp32_to_fp16_module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/torch_ext/17/fp32_to_fp16_module/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=fp32_to_fp16_module -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/17/fp32_to_fp16_module/cuda.cu -o cuda.cuda.o \n[3/3] c++ main.o cuda.cuda.o -shared -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fp32_to_fp16_module.so\nLoad time for model llm: 137.14s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.001142740249633789]\nTime to check correctness: 0.43s\nName: llm, compile: False, runtime: 27.234 ms\nTime to eval model llm, compile=False: 6.36s\n",
            "stderr": "Using /scratch/torch_ext/17 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/17/fp32_to_fp16_module...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/17/fp32_to_fp16_module/build.ninja...\nBuilding extension module fp32_to_fp16_module...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module fp32_to_fp16_module...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.001142740249633789
                ],
                "runtime": 27.23444938659668
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 29,
        "problem_id": 40,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\nLoad time for model llm: 108.44s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0004035830497741699]\nTime to check correctness: 0.22s\nName: llm, compile: False, runtime: 25.685 ms\nTime to eval model llm, compile=False: 6.61s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0004035830497741699
                ],
                "runtime": 25.684715270996094
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 30,
        "problem_id": 41,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.04s\nLoad time for model llm: 135.08s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 1.36s\nName: llm, compile: False, runtime: 54.372 ms\nTime to eval model llm, compile=False: 6.95s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 54.37186050415039
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 31,
        "problem_id": 42,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.04s\nLoad time for model llm: 0.47s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0004725456237792969]\nTime to check correctness: 7.21s\nName: llm, compile: False, runtime: 62.450 ms\nTime to eval model llm, compile=False: 6.77s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0004725456237792969
                ],
                "runtime": 62.44990921020508
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 32,
        "problem_id": 43,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.24s\nLoad time for model llm: 0.07s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0007995665073394775]\nTime to check correctness: 1.40s\nName: llm, compile: False, runtime: 1.718 ms\nTime to eval model llm, compile=False: 6.30s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0007995665073394775
                ],
                "runtime": 1.7183737754821777
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 33,
        "problem_id": 44,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.38s\nLoad time for model llm: 0.07s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0042078495025634766]\nTime to check correctness: 11.33s\nName: llm, compile: False, runtime: 4.095 ms\nTime to eval model llm, compile=False: 7.97s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0042078495025634766
                ],
                "runtime": 4.095464706420898
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 34,
        "problem_id": 48,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.04s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=mamba_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/torch_ext/35/mamba_cuda/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=mamba_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/35/mamba_cuda/cuda.cu -o cuda.cuda.o \n[3/3] c++ main.o cuda.cuda.o -shared -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o mamba_cuda.so\nLoad time for model llm: 108.37s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 1.03s\nName: llm, compile: False, runtime: 0.403 ms\nTime to eval model llm, compile=False: 11.99s\n",
            "stderr": "Using /scratch/torch_ext/35 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/35/mamba_cuda...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/35/mamba_cuda/build.ninja...\nBuilding extension module mamba_cuda...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module mamba_cuda...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.4030207693576813
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 35,
        "problem_id": 49,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.03s\n[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=mamba_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /scratch/torch_ext/15/mamba_kernels/main.cpp -o main.o \n[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output cuda.cuda.o.d -DTORCH_EXTENSION_NAME=mamba_kernels -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /opt/venv/lib/python3.12/site-packages/torch/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /opt/venv/lib/python3.12/site-packages/torch/include/TH -isystem /opt/venv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /usr/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_90,code=compute_90 -gencode=arch=compute_90,code=sm_90 --compiler-options '-fPIC' -std=c++17 -c /scratch/torch_ext/15/mamba_kernels/cuda.cu -o cuda.cuda.o \n[3/3] c++ main.o cuda.cuda.o -shared -L/opt/venv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o mamba_kernels.so\nLoad time for model llm: 133.88s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: True, Max Diff: [0.0]\nTime to check correctness: 3.12s\nName: llm, compile: False, runtime: 0.296 ms\nTime to eval model llm, compile=False: 11.52s\n",
            "stderr": "Using /scratch/torch_ext/15 as PyTorch extensions root...\nCreating extension directory /scratch/torch_ext/15/mamba_kernels...\nDetected CUDA files, patching ldflags\nEmitting ninja build file /scratch/torch_ext/15/mamba_kernels/build.ninja...\nBuilding extension module mamba_kernels...\nAllowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\nLoading extension module mamba_kernels...\n",
            "eval_results": {
                "loaded": true,
                "correct": true,
                "max_diff": [
                    0.0
                ],
                "runtime": 0.296071320772171
            },
            "timed_out": false
        }
    },
    {
        "sample_id": 36,
        "problem_id": 50,
        "results": {
            "stdout": "GPU count: 8\nLoad time for model baseline: 0.10s\nLoad time for model llm: 0.12s\nCUDA memory allocated at start: 0.0 MB\nCUDA memory reserved at start: 0.0 MB\nTested llm - Correct: False, Max Diff: [0.020929336547851562]\nTime to check correctness: 1.39s\n",
            "stderr": null,
            "eval_results": {
                "loaded": true,
                "correct": false,
                "max_diff": [
                    0.020929336547851562
                ]
            },
            "timed_out": false
        }
    }
]